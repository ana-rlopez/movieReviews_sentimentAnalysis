{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: NLP task - Sentiment analysis \n",
    "\n",
    "In this NPL task, two ML models are trained and compared: logistic regression and a CNN. Next is the code of this task.\n",
    "\n",
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import gensim\n",
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filenames and ML models' settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vars\n",
    "train_posRews_dir = '/home/ana/PycharmProjects/movieReviews/data/train/pos'\n",
    "train_negRews_dir = '/home/ana/PycharmProjects/movieReviews/data/train/neg'\n",
    "train_words_list_fname = \"trainWords_list.pkl\"\n",
    "d2v_model_fname = \"d2v.model\"\n",
    "CNN_model_fname = \"CNNmodel.h5\"\n",
    "CNN_history_fname = 'trainHistory'\n",
    "\n",
    "#test vars\n",
    "test_posRews_dir = '/home/ana/PycharmProjects/movieReviews/data/test/pos'\n",
    "test_negRews_dir = '/home/ana/PycharmProjects/movieReviews/data/test/neg'\n",
    "test_words_list_fname = \"testWords_list.pkl\"\n",
    "new_d2v_model_fname = \"new_d2v.model\"\n",
    "predictions_folder = '/home/ana/PycharmProjects/movieReviews/data/predictions'\n",
    "\n",
    "#doc2vec model settings\n",
    "d2v_settings = dict(\n",
    "    min_count = 1,\n",
    "    window = 8,\n",
    "    vector_size = 100,\n",
    "    sample = 1e-4,\n",
    "    negative= 5 ,\n",
    "    workers = 4,\n",
    "    train_epochs = 10\n",
    ")\n",
    "\n",
    "#CNN_model_settings\n",
    "CNN_settings = dict(\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions' definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, are defined the functions related with dataset (reviews) processing and extraction of (tokenized into words) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain a list of (tokenized into words, and pre-processed) documents (each one containing the sentences\n",
    "#from the corresponding review) from the text files within directory\n",
    "### A prefix is also added to identify the kind of document from the text files within directory \n",
    "#(positive or negative review)\n",
    "def getPreProcessedDocs(dir, prefix, labeled):\n",
    "\n",
    "    # list initialization\n",
    "    words = []\n",
    "    ids = []\n",
    "\n",
    "    # get tokenized sentences from positive and negative reviews\n",
    "    i = -1\n",
    "    print(\"Extracting tokenized sentences...\")\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                #print(os.path.join(root, file))\n",
    "                with open(os.path.join(root, file), 'r') as infile:\n",
    "                    text = infile.read()\n",
    "                    # pre-processing (word tokenization, plus lowercasing, remove numbers and puntuaction)\n",
    "                    if labeled==False:\n",
    "                        words.append(gensim.utils.simple_preprocess(text))\n",
    "                    #pre-processing and tagging\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        words.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(text), [prefix + '_' + str(i)]))\n",
    "                        ids.append(re.sub('\\.txt$', '', file))\n",
    "\n",
    "    return [words, ids]\n",
    "\n",
    "###Permutate the order of the tokenized sentences\n",
    "def sentencePerm(sentences):\n",
    "    shuffled_sentences = list(sentences)\n",
    "    random.shuffle(shuffled_sentences)\n",
    "    return shuffled_sentences\n",
    "\n",
    "###Extract from raw text, list of words in the proper format for input of doc2vec (and word2vec) model\n",
    "def rawText2Sentences_extraction(words_list_fname,posRews_dir,negRews_dir):\n",
    "\n",
    "    if Path(words_list_fname).exists():\n",
    "        with open(words_list_fname, \"rb\") as fp: #unpickling\n",
    "            [words, ids] = pickle.load(fp)\n",
    "\n",
    "    else:\n",
    "\n",
    "        pos_words, pos_ids = getPreProcessedDocs(posRews_dir,'POS',True)\n",
    "        neg_words, neg_ids = getPreProcessedDocs(negRews_dir, 'NEG',True)\n",
    "\n",
    "        words = pos_words + neg_words\n",
    "        ids = pos_ids + neg_ids\n",
    "\n",
    "\n",
    "        with open(words_list_fname, \"wb\") as fp: #pickling\n",
    "            pickle.dump([words, ids], fp)\n",
    "\n",
    "    return [words, ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the functions related with computing *doc2vector models* are defined. Also, a function (__docVectors2feats_extraction()__) is defined to extract from the trained doc2vec model, features in appropriate format to be fed to the sentiment analysis classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Compute a doc2vec model\n",
    "def doc2vec_extraction(d2v_model_fname, words_list_fname, train_posRews_dir, train_negRews_dir, d2v_settings):\n",
    "\n",
    "    # load trained d2v model\n",
    "    if Path(d2v_model_fname).exists() and Path(words_list_fname).exists():\n",
    "\n",
    "        d2v_model = gensim.models.Doc2Vec.load(d2v_model_fname)\n",
    "\n",
    "        with open(words_list_fname, \"rb\") as fp: #unpickling\n",
    "            ids = pickle.load(fp)[1]\n",
    "\n",
    "    #train d2v model\n",
    "    else:\n",
    "\n",
    "        train_words, ids = rawText2Sentences_extraction(words_list_fname, train_posRews_dir, train_negRews_dir)\n",
    "\n",
    "        # training of word2vec model\n",
    "        print(\"Training doc2vec model...\")\n",
    "\n",
    "        d2v_model = gensim.models.Doc2Vec(min_count=d2v_settings['min_count'], window=d2v_settings['window'],\n",
    "                                          vector_size=d2v_settings['vector_size'], sample=d2v_settings['sample'],\n",
    "                                          negative=d2v_settings['negative'], workers=d2v_settings['workers'])\n",
    "        d2v_model.build_vocab(train_words)\n",
    "        d2v_model.train(sentencePerm(train_words), total_examples=len(train_words), epochs=d2v_settings['train_epochs'])\n",
    "\n",
    "        d2v_model.save(d2v_model_fname)\n",
    "\n",
    "    return [d2v_model, ids]\n",
    "\n",
    "##Compute a doc2vec model, based on a previous model from which the vocabulary is updated\n",
    "def updatedVocab_doc2vec_extraction(d2v_model, new_d2v_model_fname, new_words_list_fname, new_posRews_dir, new_negRews_dir, d2v_settings):\n",
    "\n",
    "    # load trained new d2v model\n",
    "    if Path(new_d2v_model_fname).exists() and Path(new_words_list_fname).exists():\n",
    "\n",
    "        new_d2v_model = gensim.models.Doc2Vec.load(new_d2v_model_fname)\n",
    "\n",
    "        with open(new_words_list_fname, \"rb\") as fp: #unpickling\n",
    "            ids = pickle.load(fp)[1]\n",
    "\n",
    "    # train new d2v model\n",
    "    else:\n",
    "\n",
    "        new_train_words, ids = rawText2Sentences_extraction(new_words_list_fname, new_posRews_dir, new_negRews_dir)\n",
    "\n",
    "        print(\"Training new doc2vec model, with updated vocabulary...\")\n",
    "\n",
    "        new_d2v_model = copy.deepcopy(d2v_model)\n",
    "\n",
    "        #update vocabulary with new reviews and train model\n",
    "        new_d2v_model.build_vocab(new_train_words, update=True)\n",
    "        new_d2v_model.train(sentencePerm(new_train_words), total_examples=len(new_train_words), epochs=d2v_settings['train_epochs'])\n",
    "\n",
    "        new_d2v_model.save(new_d2v_model_fname)\n",
    "\n",
    "    return [new_d2v_model, ids]\n",
    "\n",
    "###Extract from the doc2vec vectors, vectors in a proper format to be used as input features for the classifier\n",
    "def docVectors2feats_extraction(doc2vecModel):\n",
    "    N = doc2vecModel.corpus_count\n",
    "    dim = doc2vecModel.layer1_size\n",
    "    N_pos = int(N/2) #number of positive review sentences (same as negative)\n",
    "    try:\n",
    "        N % 2 == 0\n",
    "    except:\n",
    "        print (\"N variable is not an even number\")\n",
    "\n",
    "    feats_X = np.zeros((N, dim))\n",
    "    feats_y = np.zeros((N,1))\n",
    "\n",
    "    for i in range(N_pos):\n",
    "        prefix_pos = 'POS_' + str(i)\n",
    "        prefix_neg = 'NEG_' + str(i)\n",
    "        feats_X[i,:] = doc2vecModel[prefix_pos]\n",
    "        feats_X[N_pos + i,:] = doc2vecModel[prefix_neg]\n",
    "        #1 for positive, 0 for negative\n",
    "        feats_y[i] = int(1)\n",
    "        feats_y[N_pos + i] = int(0)\n",
    "\n",
    "    return [feats_X, feats_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function creates the structure of the *CNN model* used as one of the classifiers checked in this task. This CNN is a three-layered network, that has two dense layers and one convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create CNN model\n",
    "def create_CNN_model(input_dim, input_len):\n",
    "\n",
    "    embedding_vector_length = 300 #output dimension\n",
    "    CNN_model = Sequential()\n",
    "\n",
    "    #CNN_model.add(Embedding(input_dim, embedding_vector_length, input_length=input_len))\n",
    "\n",
    "    CNN_model.add(Convolution1D(64, 3, input_shape=(input_dim,input_len), padding='same'))\n",
    "    CNN_model.add(Convolution1D(32, 3, padding='same'))\n",
    "    CNN_model.add(Convolution1D(16, 3, padding='same'))\n",
    "    CNN_model.add(Flatten())\n",
    "    CNN_model.add(Dropout(0.2))\n",
    "\n",
    "    CNN_model.add(Dense(180, activation='sigmoid'))\n",
    "    CNN_model.add(Dropout(0.2))\n",
    "\n",
    "    CNN_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return CNN_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec model training\n",
    "\n",
    "First, a doc2vec model is trained from the training set. The function __doc2vec_extraction()__ performs the training. Within this function, the following is done:\n",
    "\n",
    "1. Extract the sentences from the reviews into lists and pre-process the sentences prior to doc2vec model training (such as word-tokenization and lowercasing). A tag is added to each of the review documents (using Gensim object *TaggedDocument*), indicating if the review is positive or negative. (functions used:__getPreProcessedSentences()__ and __rawText2Sentences_extraction()__ )\n",
    "\n",
    "2. Train the doc2vec model using Gensim library (function: __doc2vec_extraction()__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokenized sentences...\n",
      "Extracting tokenized sentences...\n",
      "Training doc2vec model...\n"
     ]
    }
   ],
   "source": [
    "#doc2vec model training\n",
    "d2v_model = doc2vec_extraction(d2v_model_fname, train_words_list_fname, train_posRews_dir, train_negRews_dir, d2v_settings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, this one can be checked to see if it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('outstanding', 0.8744109869003296), ('superb', 0.8085336685180664), ('fantastic', 0.8015602231025696), ('fine', 0.783616840839386), ('terrific', 0.7822716236114502), ('brilliant', 0.7714279294013977), ('great', 0.7692869305610657), ('wonderful', 0.7537065744400024), ('stellar', 0.7373148202896118), ('exceptional', 0.7346345782279968)]\n",
      "awful:\n",
      "[('terrible', 0.8614509105682373), ('horrible', 0.8499147295951843), ('dreadful', 0.79315584897995), ('horrendous', 0.788529634475708), ('atrocious', 0.7601053714752197), ('bad', 0.7578982710838318), ('lousy', 0.7576748132705688), ('abysmal', 0.7467353343963623), ('laughable', 0.7288968563079834), ('sucks', 0.7165589928627014)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#checking that d2v model works\n",
    "w1='excellent'\n",
    "print(w1 + \":\")\n",
    "print(d2v_model.most_similar (positive=w1))\n",
    "\n",
    "w2='awful'\n",
    "print(w2 + \":\")\n",
    "print(d2v_model.most_similar (positive=w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of review classifier (sentiment analysis classifier)\n",
    "\n",
    "For this, two ML classifiers are trained and compared using validation data, and the one that performs better is chosen for prediction of the test set. The two models are a logistic regression and a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prior to training, features are extracted from the trained doc2vec model, such that those features can be used as input for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokenized sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:61: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokenized sentences...\n",
      "Training new doc2vec model, with updated vocabulary...\n"
     ]
    }
   ],
   "source": [
    "#Training of review classifier, and prediction with it:\n",
    "\n",
    "#extracting features to fed to the classifier\n",
    "\n",
    "# for training\n",
    "X_train, y_train = docVectors2feats_extraction(d2v_model)\n",
    "\n",
    "# for prediction\n",
    "new_d2v_model, ids_test = updatedVocab_doc2vec_extraction(d2v_model, new_d2v_model_fname, test_words_list_fname, test_posRews_dir, test_negRews_dir, d2v_settings)\n",
    "X_test, y_test = docVectors2feats_extraction(new_d2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model selection between logistic regression and the CNN is done using validation data. So the models are trained and:\n",
    "            * For logistic regression, the model is evaluated using crossvalidation (k-folding, with k=5)\n",
    "            * In the case of CNN, to avoid computationally expensive computations, just plain split of the training set in two parts is used: 60% is used for training, while 40% of the training set is used for validation.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Crossvalidation) Accuracy: 0.8458 (+/- 0.0173)\n",
      "Training CNN...\n",
      "CNN - Validation accuracy: 0.8461 \n"
     ]
    }
   ],
   "source": [
    "#Model selection\n",
    "\n",
    "# 1 -logistic regression\n",
    "logRegr = LogisticRegression() #initialization, default settings\n",
    "\n",
    "#cross-validation for each model\n",
    "logRegr_scores = cross_val_score(logRegr, X_train, np.ravel(y_train), cv=5)\n",
    "print(\"Logistic Regression - Crossvalidation) Accuracy: %0.4f (+/- %0.4f)\" % (logRegr_scores.mean(), logRegr_scores.std() * 2))\n",
    "\n",
    "# 2 - CNN\n",
    "\n",
    "#CNN training:\n",
    "\n",
    "#reshape data for input on the CNN model\n",
    "\n",
    "if Path(CNN_model_fname).exists() and Path(CNN_history_fname).exists():\n",
    "    CNN_model = load_model(CNN_model_fname)\n",
    "    with open(CNN_history_fname, 'rb') as fp:\n",
    "        CNN_history = pickle.load(fp)\n",
    "else:\n",
    "    X_train_resh = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "    #split train data into new_train and validation sets\n",
    "    #(cross-validation not used here, due to the computational cost)\n",
    "    [X_train_new, X_val, y_train_new, y_val] = train_test_split(X_train_resh, y_train, test_size=0.4, random_state=0)\n",
    "\n",
    "    col_len = X_train_new.shape[2]\n",
    "    row_len = X_train_new.shape[1]\n",
    "\n",
    "    CNN_model = create_CNN_model(row_len, col_len)\n",
    "    CNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #training\n",
    "    history = History()\n",
    "\n",
    "    print(\"Training CNN...\")\n",
    "    CNN_model.fit(X_train_new, y_train_new, batch_size=CNN_settings['batch_size'], epochs=CNN_settings['epochs'], verbose=CNN_settings['verbose'], validation_data=(X_val, y_val), callbacks=[history])\n",
    "\n",
    "    CNN_history = history.history\n",
    "\n",
    "    with open(CNN_history_fname, 'wb') as fp:\n",
    "        pickle.dump(CNN_history, fp)\n",
    "\n",
    "    CNN_model.save(CNN_model_fname)\n",
    "\n",
    "val_acc = CNN_history['val_acc'][-1]\n",
    "\n",
    "print('CNN - Validation accuracy: %0.4f ' % val_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models have very similar accuracy scores, therefore logistic regression is chosen since it is computationally less expensive.\n",
    "\n",
    "3. Next the logistic regression is re-trained using the whole training data.\n",
    "4. Predictions of the sentiment (positive or negative) are then predicted on the test set using the trained classifier. (predictions are formatted in this script such that: 1 - positive and 0 - negative.)\n",
    "5. Finally, classification scores are obtained. Accuracy score is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Test Accuracy: 0.8951 \n"
     ]
    }
   ],
   "source": [
    "#Model training of logistic regression (with all training data)\n",
    "logRegr.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#Prediction\n",
    "\n",
    "y_predictd_test = logRegr.predict(X_test)\n",
    "\n",
    "#Score:\n",
    "print(\"Logistic Regression - Test Accuracy: %0.4f \" % logRegr.score(X_test, np.ravel(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are then exported into a csv file using Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction export into csv file:\n",
    "pred_df = pd.DataFrame(columns=['Review_ID','Sentiment'])\n",
    "pred_df['Review_ID'] = ids_test\n",
    "pred_df['Sentiment'] = y_predictd_test #1 positive, 0 negative\n",
    "\n",
    "if not os.path.exists(predictions_folder):\n",
    "    os.makedirs(predictions_folder)\n",
    "\n",
    "pred_df.to_csv(predictions_folder + '/movieReviews_preds.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
